#!/usr/bin/env python3

import argparse
import json
import string
import pickle
from nltk.stem import PorterStemmer

class InvertedIndex:
    def __init__(self, movies_path: str, stopwords_path: str):
        self.movies_path    = movies_path
        self.stopwords_path = stopwords_path
        self.index          = {}
        self.docmap         = {}
        self.build()

    def __add_document(self, doc_id, text, translator, stemmer, stopwords):
        """
        Tokenize input text, add each token to the index with document id
        """
        # Get movie text tokens
        m_tokens = text.translate(translator).lower().split()
        m_nostop = list(set(m_tokens) - set(stopwords))
        m_stem   = [stemmer.stem(token) for token in m_nostop]

        # Add tokens to index with doc_id
        self.docmap[str(doc_id)] = m_stem

    def get_documents(self, term):
        """
        get the set of document IDs for a given token, and return them as a list
        - assume that the input term is a single word/token
        """
        stemmer = PorterStemmer()
        stemmed_term = stemmer.stem(term)
        return self.index.get(stemmed_term, [])


    def build(self):
        """
        Iterate over all movies, add them to both index and docmap
        - Concatenate title and description and use that as input
            * f"{m['title']} {m['description']}"
        - Add movie data to index with __add_document() method
        """
        with open(self.movies_path, 'r') as f:
            movies = json.load(f)
        with open(self.stopwords_path, 'r') as f:
            stopwords = f.read().splitlines()

        # strip punc, get index stems
        translator = str.maketrans('', '', string.punctuation)
        stemmer = PorterStemmer()

        # iterate over movie content, add to docmap
        for movie in movies['movies']:
            doc_id = movie['id']
            text   = f"{movie['title']} {movie['description']}"
            self.__add_document(
                    doc_id, 
                    text, 
                    translator, 
                    stemmer,
                    stopwords
            )
        
        # build out index from tokens in docmap
        for doc_id, tokens in self.docmap.items():
            for token in set(tokens):
                if token in self.index.keys():
                    self.index[token].append(str(doc_id))
                else:
                    self.index[token] = [str(doc_id)]

    def save(self):
        import os
        if not os.path.exists('cache'):
            os.mkdir('cache')
        with open('cache/index.pkl', 'wb') as f:
            pickle.dump(self.index, f)
        with open('cache/docmap.pkl', 'wb') as f:
            pickle.dump(self.docmap, f)
        
def search_movies(query: str) -> None:
    print(f'Searching for: {query}')
    with open('data/movies.json', 'r') as f:
        movies = json.load(f)
        matches = []

        # this translation strips the punctuation
        translator = str.maketrans('', '', string.punctuation)

        # iterate over movies in movie list
        for movie in movies['movies']:
            q = query.translate(translator).lower().split()
            m = movie['title'].translate(translator).lower().split()
            if any(token in m for token in q):
                matches.append(movie)
        for movie in sorted(matches, key=lambda movie: movie['id'])[:5]:
            print(f"{movie['id']}. {movie['title']}")

def main() -> None:
    parser = argparse.ArgumentParser(description="Keyword Search CLI")
    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    build_parser    = subparsers.add_parser("build", help="Build index and docmap")
    search_parser   = subparsers.add_parser("search", help="Search movies using BM25")
    search_parser.add_argument("query", type=str, help="Search query")

    args = parser.parse_args()

    match args.command:
        case "search":
            search_movies(args.query)
        case "build":
            invIndex = InvertedIndex(
                movies_path='data/movies.json',
                stopwords_path='data/stopwords.txt'
            )
            invIndex.save()
            docs = invIndex.get_documents('merida')
            if docs:
                print(f"First document ID for 'merida': {docs[0]}")
            else:
                print("Token 'merida' not found in index")
        case _:
            parser.print_help()


if __name__ == "__main__":
    main()
